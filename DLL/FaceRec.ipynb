{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "gentle-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  293\n",
      "<class 'numpy.ndarray'> (1920, 864, 3) uint8\n",
      "[(665, 788, 1220, 233)]\n",
      "BEGIN\n",
      "Writing frame 10 / 293\n",
      "Writing frame 20 / 293\n",
      "Writing frame 30 / 293\n",
      "Writing frame 40 / 293\n",
      "Writing frame 50 / 293\n",
      "Writing frame 60 / 293\n",
      "Writing frame 70 / 293\n",
      "Writing frame 80 / 293\n",
      "Writing frame 90 / 293\n",
      "Writing frame 100 / 293\n",
      "Writing frame 110 / 293\n",
      "Writing frame 120 / 293\n",
      "Writing frame 130 / 293\n",
      "Writing frame 140 / 293\n",
      "Writing frame 150 / 293\n",
      "Writing frame 160 / 293\n",
      "Writing frame 170 / 293\n",
      "Writing frame 180 / 293\n",
      "Writing frame 190 / 293\n",
      "Writing frame 200 / 293\n",
      "Writing frame 210 / 293\n",
      "Writing frame 220 / 293\n",
      "Writing frame 230 / 293\n",
      "Writing frame 240 / 293\n",
      "Writing frame 250 / 293\n",
      "Writing frame 260 / 293\n",
      "Writing frame 270 / 293\n",
      "Writing frame 280 / 293\n",
      "Writing frame 290 / 293\n",
      "ALL DONE\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy\n",
    "from PIL import Image, ImageDraw\n",
    "# This is a demo of running face recognition on a video file and saving the results to a new video file.\n",
    "#\n",
    "# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n",
    "# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n",
    "# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n",
    "\n",
    "# Open the input movie file\n",
    "path=r\"E:/switch/DL/\"\n",
    "input_movie = cv2.VideoCapture(path+\"us.mp4\")\n",
    "length = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(\"length: \",length)\n",
    "if(not input_movie.isOpened()):\n",
    "    print(\"open failed!!\")\n",
    "# Create an output movie file (make sure resolution/frame rate matches input video!)\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fourcc = cv2.VideoWriter_fourcc('M', 'P', '4', '2')\n",
    "width=int(input_movie.get(3))\n",
    "hight=int(input_movie.get(4))\n",
    "output_movie = cv2.VideoWriter('us_output.mp4', fourcc, input_movie.get(5), (width,hight))\n",
    "# Load some sample pictures and learn how to recognize them.\n",
    "lmm_image = face_recognition.load_image_file(path+\"zbt.jpg\")\n",
    "lmm_face_encoding = face_recognition.face_encodings(lmm_image)[0]\n",
    "face_locations = face_recognition.face_locations(lmm_image)\n",
    "print(type(lmm_image),lmm_image.shape,lmm_image.dtype)\n",
    "print(face_locations)\n",
    "top,right,bottom,left=face_locations[0]\n",
    "face_image = lmm_image[top:bottom, left:right]\n",
    "pil_image = Image.fromarray(face_image)\n",
    "pil_image.show()\n",
    "al_image = face_recognition.load_image_file(path+\"ds.jpg\")\n",
    "al_face_encoding = face_recognition.face_encodings(al_image)[0]\n",
    "pil_image2 = Image.fromarray(al_image)\n",
    "pil_image2.show()\n",
    "known_faces = [\n",
    "    lmm_face_encoding,\n",
    "    al_face_encoding\n",
    "]\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "frame_number = 0\n",
    "print(\"BEGIN\")\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = input_movie.read()\n",
    "\n",
    "    \n",
    "    frame_number += 1\n",
    "\n",
    "    # Quit when the input video file ends\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_frame = frame[:, :, ::-1].copy()\n",
    "    \n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        # See if the face is a match for the known face(s)\n",
    "        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.50)\n",
    "\n",
    "        # If you had more than 2 faces, you could make this logic a lot prettier\n",
    "        # but I kept it simple for the demo\n",
    "        name = None\n",
    "        if match[0]:\n",
    "            name = \"zhangbt\"\n",
    "        elif match[1]:\n",
    "            name = \"dings\"\n",
    "\n",
    "        face_names.append(name)\n",
    "\n",
    "    # Label the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        if not name:\n",
    "            continue\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 25), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    # Write the resulting image to the output video file\n",
    "    if(frame_number%10==0):\n",
    "        print(\"Writing frame {} / {}\".format(frame_number, length))\n",
    "    output_movie.write(frame)\n",
    "\n",
    "# All done!\n",
    "print(\"ALL DONE\")\n",
    "input_movie.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "detailed-independence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run program\n",
      "28.999\n",
      "(544, 960)\n",
      "in producer\n",
      "True (960, 544, 3)\n"
     ]
    }
   ],
   "source": [
    "#input_movie = cv2.VideoCapture(\"E:/jupyter-notebook/DLL/output.avi\")\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import cv2  \n",
    "import threading\n",
    "import win32gui,win32con\n",
    " \n",
    "class Producer(threading.Thread):  \n",
    "    \"\"\"docstring for ClassName\"\"\"  \n",
    "    def __init__(self,str_rtsp):  \n",
    "        super(Producer, self).__init__()  \n",
    "        self.str_rtsp = str_rtsp\n",
    "        self.play = True\n",
    "        #通过cv2中的类获取视频流操作对象cap  \n",
    "        self.cap = cv2.VideoCapture(self.str_rtsp)   \n",
    "        #调用cv2方法获取cap的视频帧（帧：每秒多少张图片）  \n",
    "        fps = self.cap.get(cv2.CAP_PROP_FPS)  \n",
    "        print(fps)  \n",
    "        #获取cap视频流的每帧大小  \n",
    "        size = (int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)),  \n",
    "                int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))  \n",
    "        print(size)  \n",
    "        #定义编码格式mpge-4  \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        #定义视频文件输入对象  \n",
    "        self.outVideo = cv2.VideoWriter('saveDir.avi',fourcc,fps,size)\n",
    "        cv2.namedWindow(\"cap video\",0);\n",
    "    def run(self):  \n",
    "        print('in producer')   \n",
    "        while True:  \n",
    "            ret,image_ = self.cap.read()\n",
    "            image=image_[:,:,::-1].copy()\n",
    "            print(ret,image.shape)\n",
    "            \n",
    "            if (ret == True):\n",
    "                if win32gui.FindWindow(None,'cap video'):\n",
    "                    cv2.imshow('cap video',image)\n",
    "                    #self.outVideo.write(image)\n",
    "                else:  \n",
    "                    self.outVideo.release()\n",
    "                    self.cap.release()  \n",
    "                    cv2.destroyAllWindows()\n",
    "                    break\n",
    "            break\n",
    "            cv2.waitKey(1)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "                self.outVideo.release()  \n",
    "                self.cap.release()  \n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "                # continue\n",
    "if __name__ == '__main__':  \n",
    "    print('run program')  \n",
    "    rtsp_str=\"E:/jupyter-notebook/DLL/output.avi\"\n",
    "    producer = Producer(rtsp_str)  \n",
    "    producer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "human-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"E:/jupyter-notebook/DLL/us_output.mp4\")\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"capture\", frame)\n",
    "    else:\n",
    "        break\n",
    "    if cv2.waitKey(30) &0xFF ==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vital-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cultural-economics",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a88ea1f3921c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "#import argparse\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "pts = deque(maxlen=64)\n",
    "\n",
    "Lower_green = np.array([110,50,50])\n",
    "Upper_green = np.array([130,255,255])\n",
    "while True:\n",
    "\tret, img=cap.read()\n",
    "\thsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "\tkernel=np.ones((5,5),np.uint8)\n",
    "\tmask=cv2.inRange(hsv,Lower_green,Upper_green)\n",
    "\tmask = cv2.erode(mask, kernel, iterations=2)\n",
    "\tmask=cv2.morphologyEx(mask,cv2.MORPH_OPEN,kernel)\n",
    "\t#mask=cv2.morphologyEx(mask,cv2.MORPH_CLOSE,kernel)\n",
    "\tmask = cv2.dilate(mask, kernel, iterations=1)\n",
    "\tres=cv2.bitwise_and(img,img,mask=mask)\n",
    "\tcnts,heir=cv2.findContours(mask.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "\tcenter = None\n",
    " \n",
    "\tif len(cnts) > 0:\n",
    "\t\tc = max(cnts, key=cv2.contourArea)\n",
    "\t\t((x, y), radius) = cv2.minEnclosingCircle(c)\n",
    "\t\tM = cv2.moments(c)\n",
    "\t\tcenter = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    " \n",
    "\t\tif radius > 5:\n",
    "\t\t\tcv2.circle(img, (int(x), int(y)), int(radius),(0, 255, 255), 2)\n",
    "\t\t\tcv2.circle(img, center, 5, (0, 0, 255), -1)\n",
    "\t\t\n",
    "\tpts.appendleft(center)\n",
    "\tfor i in range (1,len(pts)):\n",
    "\t\tif pts[i-1]is None or pts[i] is None:\n",
    "\t\t\tcontinue\n",
    "\t\tthick = int(np.sqrt(len(pts) / float(i + 1)) * 2.5)\n",
    "\t\tcv2.line(img, pts[i-1],pts[i],(0,0,225),thick)\n",
    "\t\t\n",
    "\t\n",
    "\tcv2.imshow(\"Frame\", img)\n",
    "\tcv2.imshow(\"mask\",mask)\n",
    "\tcv2.imshow(\"res\",res)\n",
    "\t\n",
    "\t\n",
    "\tk=cv2.waitKey(30) & 0xFF\n",
    "\tif k==32:\n",
    "\t\tbreak\n",
    "# cleanup the camera and close any open windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
