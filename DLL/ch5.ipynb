{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "compatible-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from scipy.fftpack import fft,ifft\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ahead-accounting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2134, -0.2015, -0.1919, -0.3539, -0.0152, -0.1017, -0.0842,  0.1741,\n",
       "         -0.0606,  0.3860],\n",
       "        [-0.4065,  0.0288, -0.2297, -0.6464,  0.1713,  0.2619, -0.3145, -0.1225,\n",
       "         -0.0079,  0.3345],\n",
       "        [-0.1719, -0.1638, -0.0548, -0.3054,  0.0793, -0.2450, -0.0977, -0.0243,\n",
       "          0.1042, -0.0279],\n",
       "        [-0.0655,  0.1287, -0.4343,  0.3586,  0.0254,  0.2035, -0.1163,  0.1022,\n",
       "         -0.2928,  0.0995],\n",
       "        [-0.2721, -0.1784,  0.0287, -0.3653,  0.0362, -0.1364, -0.0938,  0.1319,\n",
       "         -0.0185,  0.2145],\n",
       "        [-0.0247,  0.0035,  0.0427, -0.0395,  0.0997, -0.0703, -0.0635, -0.2523,\n",
       "         -0.3468,  0.1082],\n",
       "        [-0.1827,  0.3742,  0.0087, -0.5211,  0.1574,  0.4310,  0.2323, -0.0228,\n",
       "         -0.2131,  0.3930],\n",
       "        [ 0.1469, -0.0903, -0.3157, -0.0538, -0.0977,  0.0048, -0.2533,  0.3424,\n",
       "          0.0045, -0.0344],\n",
       "        [-0.1134, -0.2968,  0.1536, -0.1223,  0.3515,  0.0456, -0.2045,  0.0529,\n",
       "         -0.0310,  0.0527],\n",
       "        [-0.1269, -0.0768, -0.2990,  0.0028,  0.0170, -0.2073,  0.1793,  0.0789,\n",
       "         -0.0130,  0.0778]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=MLP()\n",
    "X=torch.normal(size=(10,20),mean=0,std=1)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "heated-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block]=block\n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X=block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vulnerable-circulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1152,  0.0811, -0.1166,  0.3599, -0.0778, -0.1648, -0.1564, -0.1097,\n",
       "          0.0263,  0.3762],\n",
       "        [-0.1831,  0.4942, -0.3682, -0.1045, -0.3149, -0.1770, -0.1376,  0.2979,\n",
       "         -0.1573,  0.5644],\n",
       "        [ 0.1419,  0.3288, -0.2088,  0.0513, -0.0448, -0.0019, -0.0485, -0.0769,\n",
       "          0.1604,  0.3819],\n",
       "        [-0.3013, -0.0275,  0.0242, -0.2029,  0.3036,  0.1372,  0.3079,  0.2586,\n",
       "          0.2608,  0.3015],\n",
       "        [-0.0786,  0.4147, -0.1808,  0.2438,  0.3697, -0.4498, -0.1408, -0.0570,\n",
       "          0.1732,  0.4524],\n",
       "        [ 0.1615,  0.5546, -0.1168, -0.1011,  0.0939,  0.1413,  0.0831,  0.0292,\n",
       "          0.2830,  0.2039],\n",
       "        [ 0.3220, -0.3814,  0.0879,  0.3373,  0.0428, -0.1796, -0.4034,  0.2381,\n",
       "          0.6042,  0.4652],\n",
       "        [-0.4978,  0.2606, -0.3775,  0.0086, -0.0580, -0.4430,  0.1749, -0.2599,\n",
       "          0.2674, -0.1226],\n",
       "        [ 0.5911,  0.0011, -0.0389,  0.3245, -0.2886, -0.1969,  0.0315,  0.2182,\n",
       "          0.4998,  0.1438],\n",
       "        [ 0.3467,  0.2175,  0.0967,  0.0829,  0.1992, -0.0020, -0.7050,  0.0957,\n",
       "          0.1436,  0.3084]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "passing-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myblock(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block]=block\n",
    "    def foward(self,X):\n",
    "        for block in self._modules.value():\n",
    "            X=block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "animated-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变。\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及`relu`和`dot`函数。\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数。\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sustainable-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4),\n",
    "                         nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rubber-overall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0114, -0.1055,  0.2234, -0.0604,  0.4429,  0.3607, -0.4041,  0.2968],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(rgnet[0][0][0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "universal-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class disdimension(nn.Module):\n",
    "    def __init__(self,input_n:int,output_n:int):\n",
    "        super().__init__()\n",
    "        self.W=torch.normal(size=(input_n,input_n,output_n),mean=0,std=1)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return torch.matmul(torch.matmul(X.T,self.W).T,X)\n",
    "class fftpar(nn.Module):\n",
    "    def __init__(self,input_n:int):\n",
    "        super().__init__()\n",
    "    def forward(self,X):\n",
    "        n=len(X)\n",
    "        ans=abs(fft(np.array(X)))\n",
    "        print(type(ans),ans.shape)\n",
    "        return torch.Tensor(ans)[0:int(n/2),0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "reasonable-classic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10, 1)\n",
      "tensor([[ 0.4173],\n",
      "        [ 0.2864],\n",
      "        [ 1.3169],\n",
      "        [ 0.0218],\n",
      "        [-0.4808],\n",
      "        [-1.1164],\n",
      "        [-0.0430],\n",
      "        [ 0.3189],\n",
      "        [ 1.1186],\n",
      "        [ 0.3859]]) tensor([0.4173, 0.2864, 1.3169, 0.0218, 0.4808])\n"
     ]
    }
   ],
   "source": [
    "POD=disdimension(10,5)\n",
    "X=torch.normal(size=(10,1),mean=0,std=1)\n",
    "fftl=fftpar(10)\n",
    "print(X,fftl(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "floating-adobe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=disdimension(10,5)\n",
    "torch.save(net.state_dict(),'netpar.txt')\n",
    "newnet=disdimension(10,5)\n",
    "newnet.load_state_dict(torch.load('netpar.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "subtle-cedar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disdimension()\n"
     ]
    }
   ],
   "source": [
    "print(newnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-slovenia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
